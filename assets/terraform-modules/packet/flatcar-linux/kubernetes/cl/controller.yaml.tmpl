---
systemd:
  units:
    - name: etcd.service
      enable: true
      contents: |
        [Unit]
        Description=etcd (System Application Container)
        Documentation=https://github.com/etcd-io/etcd
        Wants=docker.service
        After=docker.service create-etcd-config.service
        Requires=create-etcd-config.service
        [Service]
        Type=simple
        Restart=always
        RestartSec=5s
        TimeoutStartSec=0
        LimitNOFILE=40000
        EnvironmentFile=/etc/kubernetes/etcd.config
        EnvironmentFile=/etc/kubernetes/etcd.env
        ExecStartPre=-docker rm -f etcd
        ExecStartPre=sh -c "docker run -d \
          --name=etcd \
          --restart=unless-stopped \
          --log-driver=journald \
          --network=host \
          -u $(id -u \"$${USER}\"):$(id -u \"$${USER}\") \
          -v $${ETCD_DATA_DIR}:$${ETCD_DATA_DIR}:rw \
          -v $${SSL_DIR}:$${SSL_DIR}:ro \
          --env-file /etc/kubernetes/etcd.env \
          $${IMAGE_URL}:$${IMAGE_TAG}"
        ExecStart=docker logs -f etcd
        ExecStop=docker stop etcd
        ExecStopPost=docker rm etcd
        ExecStopPost=-/opt/etcd-rejoin
        [Install]
        WantedBy=multi-user.target
    - name: docker.service
      enable: true
    - name: locksmithd.service
      mask: true
    - name: coreos-metadata-sshkeys@core.service
      mask: true
    - name: wait-for-dns.service
      enable: true
      contents: |
        [Unit]
        Description=Wait for DNS entries
        Wants=systemd-resolved.service
        Before=kubelet.service etcd.service bootkube.service
        [Service]
        Restart=on-failure
        RestartSec=5s
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/sh -c 'while ! /usr/bin/grep '^[^#[:space:]]' /etc/resolv.conf > /dev/null; do sleep 1; done; /opt/wait-for-dns ${dns_zone} ${cluster_name}-private 3600'
        [Install]
        RequiredBy=kubelet.service etcd.service bootkube.service
    - name: create-etcd-config.service
      # This service will extract value of private interface from the env var file `/run/metadata/flatcar`.
      # And then assign it to the variables that which will be stored in file `/etc/kubernetes/etcd.config`,
      # this file is sourced by etcd service.
      enable: true
      contents: |
        [Unit]
        Description=Create a etcd env file to be used by etcd to listen on private interface
        Before=etcd.service
        Requires=coreos-metadata.service
        After=coreos-metadata.service
        [Service]
        EnvironmentFile=/run/metadata/flatcar
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/sh -c 'echo "ETCD_LISTEN_CLIENT_URLS=https://$COREOS_PACKET_IPV4_PRIVATE_0:2379" > /etc/kubernetes/etcd.config && echo "ETCD_LISTEN_PEER_URLS=https://$COREOS_PACKET_IPV4_PRIVATE_0:2380" >> /etc/kubernetes/etcd.config && echo "ETCD_LISTEN_METRICS_URLS=http://$COREOS_PACKET_IPV4_PRIVATE_0:2381" >> /etc/kubernetes/etcd.config'
        [Install]
        RequiredBy=etcd.service
    - name: coreos-metadata.service
      enable: true
      contents: |
        [Unit]
        Description=Flatcar Container Linux Metadata Agent
        [Service]
        Type=oneshot
        RemainAfterExit=true
        Restart=on-failure
        RestartSec=10s
        Environment=COREOS_METADATA_OPT_PROVIDER=--cmdline
        ExecStart=/usr/bin/coreos-metadata $${COREOS_METADATA_OPT_PROVIDER} --attributes=/run/metadata/flatcar
        [Install]
        RequiredBy=metadata.target
    - name: kubelet.service
      enable: true
      contents: |
        [Unit]
        Description=Kubelet
        Requires=coreos-metadata.service
        After=coreos-metadata.service
        Wants=rpc-statd.service
        [Service]
        EnvironmentFile=/run/metadata/flatcar
        ConditionPathExists=/etc/kubernetes/kubeconfig
        EnvironmentFile=/etc/kubernetes/kubelet.env
        ExecStartPre=/bin/mkdir -p /var/lib/kubelet/volumeplugins
        ExecStartPre=/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/bash -c "grep 'certificate-authority-data' /etc/kubernetes/kubeconfig | awk '{print $2}' | base64 -d > /etc/kubernetes/ca.crt"
        ExecStartPre=/etc/kubernetes/configure-kubelet-cgroup-driver
        ExecStartPre=-docker rm -f kubelet
        ExecStartPre=docker run -d \
          --name=kubelet \
          --restart=unless-stopped \
          --log-driver=journald \
          --network=host \
          --pid=host \
          --privileged \
          -v /dev:/dev:rw \
          -v /etc/cni/net.d:/etc/cni/net.d:ro \
          -v /etc/kubernetes:/etc/kubernetes:ro \
          -v /etc/machine-id:/etc/machine-id:ro \
          -v /lib/modules:/lib/modules:ro \
          -v /run:/run:rw \
          -v /sys:/sys:rw \
          -v /opt/cni/bin:/opt/cni/bin:ro \
          -v /usr/lib/os-release:/etc/os-release:ro \
          -v /var/lib/calico:/var/lib/calico:ro \
          -v /var/lib/cni:/var/lib/cni:rw \
          -v /var/lib/docker:/var/lib/docker:rw \
          -v /var/log/pods:/var/log/pods:rw \
          --mount type=bind,source=/mnt,target=/mnt,bind-propagation=rshared \
          --mount type=bind,source=/var/lib/kubelet,target=/var/lib/kubelet,bind-propagation=rshared \
          $${KUBELET_IMAGE_URL}:$${KUBELET_IMAGE_TAG} \
          --node-ip=$${COREOS_PACKET_IPV4_PRIVATE_0} \
          --anonymous-auth=false \
          --authentication-token-webhook \
          --authorization-mode=Webhook \
          --client-ca-file=/etc/kubernetes/ca.crt \
          --cluster_dns=${k8s_dns_service_ip} \
          --cluster_domain=${cluster_domain_suffix} \
          --cloud-provider=external \
          --cni-conf-dir=/etc/cni/net.d \
          --config=/etc/kubernetes/kubelet.config \
          --exit-on-lock-contention \
          %{~ if enable_tls_bootstrap ~}
          --kubeconfig=/var/lib/kubelet/kubeconfig \
          --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig \
          --rotate-certificates \
          %{~ else ~}
          --kubeconfig=/etc/kubernetes/kubeconfig \
          %{~ endif ~}
          --lock-file=/var/run/lock/kubelet.lock \
          --network-plugin=cni \
          --node-labels=$${NODE_LABELS} \
          --node-labels=lokomotive.alpha.kinvolk.io/public-ipv4=$${COREOS_PACKET_IPV4_PUBLIC_0} \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --read-only-port=0 \
          --register-with-taints=$${NODE_TAINTS} \
          --address=$${COREOS_PACKET_IPV4_PRIVATE_0} \
          --volume-plugin-dir=/var/lib/kubelet/volumeplugins
        ExecStart=docker logs -f kubelet
        ExecStop=docker stop kubelet
        ExecStopPost=docker rm kubelet
        Restart=always
        RestartSec=5
        [Install]
        WantedBy=multi-user.target
    - name: bootkube.service
      contents: |
        [Unit]
        Description=Bootstrap a Kubernetes cluster
        ConditionPathExists=!/opt/bootkube/init_bootkube.done
        [Service]
        Type=oneshot
        RemainAfterExit=true
        WorkingDirectory=/opt/bootkube
        ExecStart=/opt/bootkube/bootkube-start
        ExecStartPost=/bin/touch /opt/bootkube/init_bootkube.done
        [Install]
        WantedBy=multi-user.target
    - name: "iptables-restore.service"
      enabled: true
      enable: true
    - name: "ip6tables-restore.service"
      enabled: true
      enable: true
storage:
  files:
    - path: /etc/kubernetes/kubeconfig
      filesystem: root
      mode: 0644
      contents:
        inline: |
          ${kubeconfig}
    - path: /etc/kubernetes/kubelet.env
      filesystem: root
      mode: 0644
      contents:
        inline: |
          KUBELET_IMAGE_URL=quay.io/kinvolk/kubelet
          KUBELET_IMAGE_TAG=v1.20.5-${os_arch}
          NODE_LABELS="node.kubernetes.io/master,node.kubernetes.io/controller=true"
          NODE_TAINTS="node-role.kubernetes.io/master=:NoSchedule"
    - path: /etc/kubernetes/etcd.env
      filesystem: root
      mode: 0644
      contents:
        inline: |
          IMAGE_TAG=v3.4.15${etcd_arch_tag_suffix}
          IMAGE_URL=quay.io/coreos/etcd
          SSL_DIR=/etc/ssl/etcd
          USER=etcd
          ETCD_DATA_DIR=/var/lib/etcd
          ETCD_NAME=${etcd_name}
          ETCD_ADVERTISE_CLIENT_URLS=https://${etcd_domain}:2379
          ETCD_INITIAL_ADVERTISE_PEER_URLS=https://${etcd_domain}:2380
          ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379
          ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380
          ETCD_LISTEN_METRICS_URLS=http://0.0.0.0:2381
          ETCD_INITIAL_CLUSTER=${etcd_initial_cluster}
          ETCD_STRICT_RECONFIG_CHECK=true
          ETCD_TRUSTED_CA_FILE=/etc/ssl/etcd/etcd/server-ca.crt
          ETCD_CERT_FILE=/etc/ssl/etcd/etcd/server.crt
          ETCD_KEY_FILE=/etc/ssl/etcd/etcd/server.key
          ETCD_CLIENT_CERT_AUTH=true
          ETCD_PEER_TRUSTED_CA_FILE=/etc/ssl/etcd/etcd/peer-ca.crt
          ETCD_PEER_CERT_FILE=/etc/ssl/etcd/etcd/peer.crt
          ETCD_PEER_KEY_FILE=/etc/ssl/etcd/etcd/peer.key
          ETCD_PEER_CLIENT_CERT_AUTH=true
          ${etcd_arch_options}
    - path: /etc/sysctl.d/max-user-watches.conf
      filesystem: root
      contents:
        inline: |
          fs.inotify.max_user_watches=16184
    - path: /opt/bootkube/bootkube-start
      filesystem: root
      mode: 0544
      user:
        id: 500
      group:
        id: 500
      contents:
        inline: |
          #!/bin/bash
          # Wrapper for bootkube start
          set -e
          # Move experimental manifests
          [ -n "$(ls /opt/bootkube/assets/manifests-*/* 2>/dev/null)" ] && mv /opt/bootkube/assets/manifests-*/* /opt/bootkube/assets/manifests && rm -rf /opt/bootkube/assets/manifests-*
          exec docker run \
            -v /opt/bootkube/assets:/assets:ro \
            -v /etc/kubernetes:/etc/kubernetes:rw \
            --network=host \
            quay.io/kinvolk/bootkube:v0.14.0-helm3-${os_arch} \
            /bootkube start --asset-dir=/assets
    - path: /etc/tmpfiles.d/etcd-wrapper.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          d    /var/lib/etcd 0700 etcd etcd - -
    - path: /opt/etcd-rejoin
      filesystem: root
      mode: 0555
      contents:
        inline: |
          #!/bin/bash
          set -eou pipefail
          # Rejoin a cluster as fresh node when etcd cannot join
          # (e.g., after repovisioning, crashing or node being down).
          # Set ExecStopPost=-/opt/etcd-rejoin to run when etcd failed and
          # use env vars of etcd.service.
          # Skip if not provisioned
          if [ ! -d "/etc/ssl/etcd/" ]; then exit 0; fi
          # or got stopped.
          if [ "$EXIT_CODE" = "killed" ]; then exit 0; fi
          now=$(date +%s)
          if [ -f /var/lib/etcd-last-fail ]; then
            last=$(cat /var/lib/etcd-last-fail)
          else
            last=0
          fi
          echo "$now" > /var/lib/etcd-last-fail
          let "d = $now - $last"
          # Skip and restart regularly if it does not fail within 120s.
          if [ "$d" -gt 120 ]; then exit 0; fi
          export ETCDCTL_API=3
          urls=$(echo "$ETCD_INITIAL_CLUSTER" | tr "," "\n" | cut -d "=" -f 2 | tr "\n" "," | head -c -1)
          # $$ for terraform
          endpoints="$${urls//2380/2379}"
          ARGS="--cacert=/etc/ssl/etcd/etcd-client-ca.crt --cert=/etc/ssl/etcd/etcd-client.crt --key=/etc/ssl/etcd/etcd-client.key --endpoints=$endpoints"
          # Check if unhealthy (should be because etcd is not running)
          unhealty=$((etcdctl endpoint health $ARGS 2> /dev/stdout | grep "is unhealthy" | grep "$ETCD_NAME") || true)
          if [ -z "$unhealty" ]; then exit 0; fi
          # Remove old ID if still exists
          ID=$((etcdctl member list $ARGS | grep "$ETCD_NAME" | cut -d "," -f 1) || true)
          if [ ! -z "$ID" ]; then
            etcdctl member remove "$ID" $ARGS
          fi
          # Re-add as new member
          etcdctl member add "$ETCD_NAME" --peer-urls="$ETCD_INITIAL_ADVERTISE_PEER_URLS" $ARGS
          # Join fresh without state
          mv /var/lib/etcd "/var/lib/etcd-bkp-$(date +%s)" || true
          install -m 700 -o etcd -g etcd -d /var/lib/etcd
          if [ -z "$(grep ETCD_INITIAL_CLUSTER_STATE=existing /etc/kubernetes/etcd.env)" ]; then
            echo ETCD_INITIAL_CLUSTER_STATE=existing >> /etc/kubernetes/etcd.env
            # Apply change
            systemctl daemon-reload
          fi
          # Restart unit (yes, within itself)
          systemctl restart etcd &
    - path: /var/lib/iptables/rules-save
      filesystem: root
      mode: 0644
      contents:
        inline: |
          *filter
          :INPUT DROP [0:0]
          :FORWARD ACCEPT [0:0]
          :OUTPUT ACCEPT [0:0]
          -A INPUT -i lo -j ACCEPT
          -A INPUT -p tcp --dport 22 -j ACCEPT
          # Use 10.0.0.0/8 as this is Packet private network CIDR.
          # It will be closed more tightly via Calico, which rules are easy to update.
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 179 -j ACCEPT
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 2379 -j ACCEPT
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 2380 -j ACCEPT
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 2381 -j ACCEPT
          -A INPUT -p tcp --dport 6443 -j ACCEPT
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 10250 -j ACCEPT
          -A INPUT -s 10.0.0.0/8 -p tcp --dport 10256 -j ACCEPT
          -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
          COMMIT
    - path: /var/lib/ip6tables/rules-save
      filesystem: root
      mode: 0644
      contents:
        inline: |
          *filter
          :INPUT DROP [0:0]
          :FORWARD DROP [0:0]
          :OUTPUT ACCEPT [0:0]
          -A INPUT -i lo -j ACCEPT
          -A INPUT -p tcp --dport 22 -j ACCEPT
          -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
          COMMIT
    - path: /etc/kubernetes/configure-kubelet-cgroup-driver
      filesystem: root
      mode: 0744
      contents:
        inline: |
          #!/bin/bash
          set -e
          readonly docker_cgroup_driver="$(docker info -f '{{.CgroupDriver}}')"
          cat <<EOF >/etc/kubernetes/kubelet.config
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          cgroupDriver: "$${docker_cgroup_driver}"
          EOF
    - path: /opt/wait-for-dns
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/bash
          if [[ $# -ne 3 ]]; then
              echo "Usage: $0 <zone> <record> <max_attempts>"
              exit 1
          fi
          zone=$1
          record=$2
          max_attempts=$3
          echo "Figuring out the nameservers for $zone"
          nameservers=""
          counter=0
          while [[ $counter -lt $max_attempts ]]; do
              out=$(dig +short +timeout=2 "$zone" ns)
              ret=$?
              if [[ $ret -eq 0 && "$out" != "" ]]; then
                  nameservers=$out
                  break
              fi
              if [[ "$out" = "" ]]; then
                 echo "No nameservers found for $zone"
              else
                 echo "dig failed with exit code $ret: $out"
              fi
              sleep 1
              counter=$((counter+1))
          done
          if [[ "$nameservers" == "" ]]; then
              echo "Could not resolve nameservers for $zone"
              exit 1
          fi
          for ns in $nameservers; do
              echo "Polling $ns for $record.$zone..."
              counter=0
              ok=false
              while [[ $counter -lt $max_attempts ]]; do
                  out=$(dig +short +timeout=2 @"$ns" "$record"."$zone" a)
                  ret=$?
                  if [[ $ret -eq 0 && "$out" != "" ]]; then
                      echo "Looks good!"
                      ok=true
                      break
                  fi
                  echo "Not available yet"
                  sleep 1
                  counter=$((counter+1))
              done
              if ! $ok; then
                  echo "$record.$zone didn't become available within the allowed time"
                  exit 1
              fi
          done
          echo "$record.$zone is available on all nameservers"
          exit 0
    - path: /etc/docker/daemon.json
      filesystem: root
      mode: 0500
      contents:
        inline: |
          {
            "live-restore": true,
            "log-opts": {
              "max-size": "100m",
              "max-file": "3"
            }
          }
passwd:
  users:
  - name: core
    ssh_authorized_keys: ${ssh_keys}
